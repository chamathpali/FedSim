{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For more information refer - https://github.com/chamathpali/Fed-Goodreads\n",
    "\n",
    "import gzip\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = './'\n",
    "def load_data(file_name, head = 500):\n",
    "    count = 0\n",
    "    data = []\n",
    "    with gzip.open(file_name) as fin:\n",
    "        for l in fin:\n",
    "            d = json.loads(l)\n",
    "            count += 1\n",
    "            data.append(d)\n",
    "            \n",
    "            # break if reaches the 100th line\n",
    "            if (head is not None) and (count > head):\n",
    "                break\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>sent</th>\n",
       "      <th>has_spoiler</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1183653</th>\n",
       "      <td>58fe4880f1c5746d7311867093747775</td>\n",
       "      <td>They complain a lot and almost look stupid and...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2101183</th>\n",
       "      <td>abf8fe152026f9b2ac3581c956b5782b</td>\n",
       "      <td>I was drawn in to the story pretty easily and ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84649</th>\n",
       "      <td>77008aaf6eb579f3c87a62fd59f936fd</td>\n",
       "      <td>I'm glad the writers didn't completely erase h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2031516</th>\n",
       "      <td>e3fd6992ad541c59468adff19e8c94d5</td>\n",
       "      <td>People usually don't make the best decisions w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993814</th>\n",
       "      <td>2bea09c443d38d253711f23d7b331927</td>\n",
       "      <td>I'm surprised that there were some recycled id...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1153749</th>\n",
       "      <td>8f61979007ffdef4920cf2e3384e2030</td>\n",
       "      <td>Even though it was a great story (although som...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660066</th>\n",
       "      <td>24d04e3d0b2fe7ef8366de93618eb904</td>\n",
       "      <td>She remains one of my favourite children's aut...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296231</th>\n",
       "      <td>af941aae76ff1b9c9b256cbb8ebc2aef</td>\n",
       "      <td>Julius kissed him again.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611524</th>\n",
       "      <td>46e23d760843b8c9a4108a55cd92413d</td>\n",
       "      <td>Or being post-TWS, except that Shield still ex...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178945</th>\n",
       "      <td>c66ec6d215a6d6a6ced2398ebf56ce70</td>\n",
       "      <td>\"Delicate, beautiful, fragile.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785329</th>\n",
       "      <td>9d99d1c0336449e5513e2b5a4a8703fc</td>\n",
       "      <td>4-4.5 I think I have a lot to say, whenever I ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79591</th>\n",
       "      <td>7ceac7249b88317ea7403e1021170baa</td>\n",
       "      <td>It can't be because apologizing doesn't erase ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2218924</th>\n",
       "      <td>848e17ccddc6dd4c241088f54c06c19a</td>\n",
       "      <td>It's funny, and as a Philly girl it made me re...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843611</th>\n",
       "      <td>ce0996178c46d462b9321725e94551c9</td>\n",
       "      <td>They expose a lot about the food industry that...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1691114</th>\n",
       "      <td>fe4b4225b3abb22d1fa8a2f084afd3d1</td>\n",
       "      <td>This is the first in the series and so I assum...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2270482</th>\n",
       "      <td>28d6811ede0fc3484cc09dd746864db7</td>\n",
       "      <td>Because I had just finished reading A Thousand...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1518988</th>\n",
       "      <td>395934a50d7a4099c0e846a956cdbf30</td>\n",
       "      <td>Much later on, though, I figured out that the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971263</th>\n",
       "      <td>1de96145769ccdd2e9edfc83bb73eeb6</td>\n",
       "      <td>Which were met, but not exceeded.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107514</th>\n",
       "      <td>c4f655f2ed79599a9259d7762313f3f3</td>\n",
       "      <td>For the record: the \"chemical gardens\" bit sti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2260397</th>\n",
       "      <td>e0057553ee88e92fa7f626e49d506d29</td>\n",
       "      <td>I must admit that you excited me a bit.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     user  \\\n",
       "1183653  58fe4880f1c5746d7311867093747775   \n",
       "2101183  abf8fe152026f9b2ac3581c956b5782b   \n",
       "84649    77008aaf6eb579f3c87a62fd59f936fd   \n",
       "2031516  e3fd6992ad541c59468adff19e8c94d5   \n",
       "1993814  2bea09c443d38d253711f23d7b331927   \n",
       "1153749  8f61979007ffdef4920cf2e3384e2030   \n",
       "660066   24d04e3d0b2fe7ef8366de93618eb904   \n",
       "296231   af941aae76ff1b9c9b256cbb8ebc2aef   \n",
       "611524   46e23d760843b8c9a4108a55cd92413d   \n",
       "178945   c66ec6d215a6d6a6ced2398ebf56ce70   \n",
       "785329   9d99d1c0336449e5513e2b5a4a8703fc   \n",
       "79591    7ceac7249b88317ea7403e1021170baa   \n",
       "2218924  848e17ccddc6dd4c241088f54c06c19a   \n",
       "843611   ce0996178c46d462b9321725e94551c9   \n",
       "1691114  fe4b4225b3abb22d1fa8a2f084afd3d1   \n",
       "2270482  28d6811ede0fc3484cc09dd746864db7   \n",
       "1518988  395934a50d7a4099c0e846a956cdbf30   \n",
       "971263   1de96145769ccdd2e9edfc83bb73eeb6   \n",
       "107514   c4f655f2ed79599a9259d7762313f3f3   \n",
       "2260397  e0057553ee88e92fa7f626e49d506d29   \n",
       "\n",
       "                                                      sent  has_spoiler  \n",
       "1183653  They complain a lot and almost look stupid and...            0  \n",
       "2101183  I was drawn in to the story pretty easily and ...            0  \n",
       "84649    I'm glad the writers didn't completely erase h...            0  \n",
       "2031516  People usually don't make the best decisions w...            0  \n",
       "1993814  I'm surprised that there were some recycled id...            0  \n",
       "1153749  Even though it was a great story (although som...            0  \n",
       "660066   She remains one of my favourite children's aut...            0  \n",
       "296231                            Julius kissed him again.            0  \n",
       "611524   Or being post-TWS, except that Shield still ex...            0  \n",
       "178945                      \"Delicate, beautiful, fragile.            0  \n",
       "785329   4-4.5 I think I have a lot to say, whenever I ...            0  \n",
       "79591    It can't be because apologizing doesn't erase ...            0  \n",
       "2218924  It's funny, and as a Philly girl it made me re...            0  \n",
       "843611   They expose a lot about the food industry that...            0  \n",
       "1691114  This is the first in the series and so I assum...            0  \n",
       "2270482  Because I had just finished reading A Thousand...            0  \n",
       "1518988  Much later on, though, I figured out that the ...            0  \n",
       "971263                   Which were met, but not exceeded.            0  \n",
       "107514   For the record: the \"chemical gardens\" bit sti...            0  \n",
       "2260397            I must admit that you excited me a bit.            0  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = load_data('goodreads_reviews_spoiler.json.gz', 200000)\n",
    "# display(np.random.choice(reviews))\n",
    "print(len(reviews))\n",
    "# print(reviews[0])\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "all_data = []\n",
    "for review in reviews:\n",
    "    for sent in review[\"review_sentences\"]:\n",
    "        \n",
    "        all_data.append({\"user\": review[\"user_id\"], \"sent\": sent[1], \"has_spoiler\": sent[0] })\n",
    "\n",
    "        \n",
    "import pandas as pd         \n",
    "all_reviews = pd.DataFrame(all_data)\n",
    "all_reviews = all_reviews.sample(frac=1)\n",
    "\n",
    "all_reviews.size\n",
    "all_reviews.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2457426, 2517)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df=1000)\n",
    "vectorized = vectorizer.fit_transform(all_reviews[\"sent\"])\n",
    "vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['58fe4880f1c5746d7311867093747775'\n",
      "  'They complain a lot and almost look stupid and incompetent.' 0]\n",
      " ['abf8fe152026f9b2ac3581c956b5782b'\n",
      "  'I was drawn in to the story pretty easily and stayed fairly engaged throughout, though I will admit I got a little bored there towards the end.'\n",
      "  0]\n",
      " ['77008aaf6eb579f3c87a62fd59f936fd'\n",
      "  \"I'm glad the writers didn't completely erase her history.\" 0]\n",
      " ...\n",
      " ['fd5a89a93c4c94b35b1a480eaaad2166'\n",
      "  'Though I like some parts of the story there are others that seems iffy for me:'\n",
      "  0]\n",
      " ['1c45a55c17e09a1a9148b8a401274fcd' 'The end!' 0]\n",
      " ['2138f2f7e5d766220af5a36e7dc0c9e1'\n",
      "  'Andie has her whole summer planned out.' 0]]\n"
     ]
    }
   ],
   "source": [
    "pro_reviews = vectorized.toarray()\n",
    "new_reviews = all_reviews.to_numpy()\n",
    "print(new_reviews)\n",
    "\n",
    "users_raw_data = {}\n",
    "for idx, review in enumerate(new_reviews):\n",
    "    user = review[0]\n",
    "    if(user not in users_raw_data ):\n",
    "        users_raw_data[review[0]] = { \"id\": user, \"x\":[], \"y\":[], \"num_samples\": 0, \"spoiler_count\":0, \"n_spoiler_count\":0 }\n",
    "    \n",
    "    users_raw_data[user][\"y\"].append(review[2])\n",
    "    users_raw_data[user][\"x\"].append(pro_reviews[idx])\n",
    "        \n",
    "    users_raw_data[user][\"num_samples\"] += 1\n",
    "    if(review[2] == 1):\n",
    "        users_raw_data[user][\"spoiler_count\"] += 1\n",
    "    else:\n",
    "        users_raw_data[user][\"n_spoiler_count\"] += 1\n",
    "\n",
    "# print(users_raw_data[\"ab2fadb5c7bbe55c80406d2b3692e969\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 2 1 3 0 5 4]\n",
      "7\n",
      "[1 0 1 0 0]\n",
      "[1 0]\n",
      "[3 0 2 1]\n",
      "4\n",
      "[1 0 0]\n",
      "[0]\n",
      "[3 1 2 4 5 0 6 7]\n",
      "8\n",
      "[0 0 0 1 1 0]\n",
      "[1 1]\n",
      "[5 2 3 4 1 0 6]\n",
      "7\n",
      "[1 0 0 1 0]\n",
      "[0 1]\n",
      "[2 3 4 1 0]\n",
      "5\n",
      "[1 1 1 0]\n",
      "[0]\n",
      "100\n",
      "[5, 3, 6, 5, 4, 4, 2, 4, 3, 3, 4, 5, 4, 6, 3, 1, 4, 5, 4, 2, 3, 4, 1, 5, 3, 3, 4, 5, 4, 4, 4, 3, 3, 4, 2, 2, 4, 4, 4, 5, 5, 3, 4, 4, 2, 3, 4, 4, 4, 2, 2, 1, 2, 4, 3, 1, 2, 2, 4, 1, 4, 3, 2, 4, 4, 4, 1, 4, 4, 4, 2, 3, 4, 4, 2, 6, 4, 5, 2, 4, 3, 4, 4, 4, 4, 5, 4, 2, 4, 3, 6, 4, 6, 2, 4, 4, 6, 1, 5, 5]\n",
      "[2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2, 2, 2, 1, 1, 2, 1, 1, 2, 2, 2, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "user_list = []\n",
    "user_data_train = {}\n",
    "user_data_test = {}\n",
    "\n",
    "totals_train = []\n",
    "totals_test = []\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "sequence = [i for i in range(10,21)]\n",
    "# sequence2 = [i for i in range(10,21)]\n",
    "\n",
    "count = 0\n",
    "# print(perm)\n",
    "for user in users_raw_data:\n",
    "    if(count >= 100 or users_raw_data[user][\"spoiler_count\"] < 3 or users_raw_data[user][\"n_spoiler_count\"] < 3 ):\n",
    "        continue\n",
    "    \n",
    "    sequence = [i for i in range(1, 5 )]\n",
    "    sequence2 = [i for i in range(1,  5 )]\n",
    "    \n",
    "    limit_spoiler = random.choice(sequence)\n",
    "    limit_non    = random.choice(sequence2)\n",
    "#     print(limit_non)\n",
    "#     print(limit_spoiler)\n",
    "    \n",
    "    user_list.append(user)\n",
    "\n",
    "    \n",
    "    user_data_raw_ = users_raw_data[user]\n",
    "    \n",
    "    user_data = {\"x\":[], \"y\": []}\n",
    "    \n",
    "    for x,y in zip(user_data_raw_[\"x\"], user_data_raw_[\"y\"]):\n",
    "        if(y == 0 and limit_non > 0 ):\n",
    "            user_data[\"x\"].append(x)\n",
    "            user_data[\"y\"].append(y)\n",
    "            limit_non -= 1\n",
    "        \n",
    "        if(y == 1 and limit_spoiler > 0 ):\n",
    "            user_data[\"x\"].append(x)\n",
    "            user_data[\"y\"].append(y)\n",
    "            limit_spoiler -= 1\n",
    "            \n",
    "#     print(y)\n",
    "    \n",
    "#     user_data[\"x\"] = user_data[\"x\"][:limit]\n",
    "#     user_data[\"y\"] = user_data[\"y\"][:limit]\n",
    "    \n",
    "    perm = np.random.permutation(len(user_data[\"x\"]))\n",
    "    user_data[\"x\"] = np.array(user_data[\"x\"])\n",
    "    user_data[\"y\"] = np.array(user_data[\"y\"])\n",
    "    \n",
    "    user_data[\"x\"] = user_data[\"x\"][perm]\n",
    "    user_data[\"y\"] = user_data[\"y\"][perm]\n",
    "        \n",
    "    split = int(len(user_data[\"x\"])*0.8)\n",
    "\n",
    "    train_x = user_data[\"x\"][:split]\n",
    "    train_y = user_data[\"y\"][:split]\n",
    "    \n",
    "    test_x  = user_data[\"x\"][split:]\n",
    "    test_y  = user_data[\"y\"][split:]\n",
    "    \n",
    "    if(count < 5):\n",
    "        print(perm)\n",
    "        print(len(user_data[\"x\"]))\n",
    "        print(train_y)\n",
    "        print(test_y)\n",
    "    \n",
    "    user_data_train[user] = {\"y\": np.array(train_y).tolist(), \"x\": np.array(train_x).tolist()}\n",
    "    user_data_test[user] = {\"y\":  np.array(test_y).tolist(), \"x\":  np.array(test_x).tolist()}\n",
    "    \n",
    "    count += 1\n",
    "    \n",
    "    totals_train.append(len(train_y))\n",
    "    totals_test.append(len(test_y))\n",
    "#     print(users_raw_data[user][\"num_samples\"])\n",
    "# print(user_list)\n",
    "print(len(totals_train))\n",
    "print(totals_train)\n",
    "print(totals_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "train_dict = {\"users\": user_list, \"user_data\": user_data_train, \"num_samples\": totals_train}\n",
    "test_dict = {\"users\": user_list, \"user_data\": user_data_test, \"num_samples\": totals_test}\n",
    "# totals_train\n",
    "print(len(totals_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/train/train.json', 'w') as json_file:\n",
    "    json.dump(train_dict, json_file)\n",
    "with open('data/test/test.json', 'w') as json_file:\n",
    "    json.dump(test_dict, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}